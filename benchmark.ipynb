{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place hard images in the benchmark folder\n",
    "\n",
    "This notebook will run over only the images in benchmark/\n",
    "\n",
    "Images in this folder are there because they are difficult or inconsistent.\n",
    "\n",
    "If a good result has been obtained, they may be added to examples.txt\n",
    "\n",
    "We can remove any such examples after finding a good solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import ctpy as ct\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Use the API key from .env OR set it as None\n",
    "try:\n",
    "    api_key = os.getenv('API_KEY')\n",
    "except:\n",
    "    api_key = None\n",
    "\n",
    "gemini_key = os.getenv('VERTEX')\n",
    "\n",
    "# name will determine the name used in raw_chapeters and final_chapters\n",
    "# url is used only for scrape()\n",
    "test = ct.issue(\n",
    "    name='benchmark',\n",
    "    url = 'benchmark', # pull images from benchmark folder\n",
    "    api_key = api_key,\n",
    "    gemini_key=gemini_key,\n",
    "    )\n",
    "\n",
    "# this will delete everything in raw_chapters/benchmark\n",
    "# comment it out to save gpt responses upon restart\n",
    "test.clear_directories()\n",
    "\n",
    "# you can rerun this everytime\n",
    "test.scrape()\n",
    "\n",
    "'''\n",
    "increasing scale_factor can help\n",
    "BUT it makes it more expensive\n",
    "\n",
    "Play around with it if you want to\n",
    "'''\n",
    "test.downsample(scale_factor=1.0)\n",
    "\n",
    "#! gcloud config set project comictranslator-407423\n",
    "#! gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages can be combined, but we will leave them separate in the test\n",
    "\n",
    "#test.combine_pages(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We need to draw boxes around all text in images\n",
    "I tried to automate it, but pytesseract did not perform well for this.\n",
    "\n",
    "Use next to go to the next image\n",
    "\n",
    "At the end things will lag because it is \n",
    "doing OKR Optical Korean Recognition on each image\n",
    "'''\n",
    "drawer = ct.BoxDrawer(test)\n",
    "if False:\n",
    "    drawer.draw()\n",
    "    drawer.save(name='benchmark')\n",
    "else:\n",
    "    drawer.load(name='benchmark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.add_boxes_to_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawer.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "test.perform_ocr_on_all_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This method combines the full color image with the image used for OKR\n",
    "\n",
    "This also makes it more expensive.\n",
    "This might not be needed, but I do think it helps a lot.\n",
    "'''\n",
    "# some different input methods\n",
    "if False:\n",
    "    test.tile_page_for_gpt()\n",
    "elif False:\n",
    "    test.copy_to_gpt_images()\n",
    "else:\n",
    "    test.add_boxes_to_images()\n",
    "    test.combine_gpt_and_ocr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making images larger has improved performance, but it costs more\n",
    "# better to try to find other ways of improving quality\n",
    "#test.resize_gpt(scale=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pipe those images to the AI\n",
    "prompt will be saved in raw_chapters/benchmark/prompts\n",
    "response saved in raw_chapters/benchmark/response -and- /text\n",
    "\n",
    "To re-do just one of the pages, delete the file in...\n",
    "\n",
    "raw_chapters/benchmark/text/page_num_#.txt\n",
    "\n",
    "'''\n",
    "\n",
    "# to run gemini you will need to run these commands in the terminal\n",
    "# gemini_key = os.getenv('VERTEX')\n",
    "# ! gcloud config set project comictranslator-407423\n",
    "# ! gcloud auth login\n",
    "\n",
    "if True:\n",
    "    test.translate()\n",
    "else:\n",
    "    test.translate(model='gemini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf is formatted automatically\n",
    "# It can be weird if your images are too big or small\n",
    "test.make_pdf()\n",
    "\n",
    "# this test may report 'failed response'\n",
    "# check the response, sometimes GPT just says no.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comic_translator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
